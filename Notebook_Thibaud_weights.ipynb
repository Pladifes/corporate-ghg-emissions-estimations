{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functions.loading import load_data\n",
    "\n",
    "from functions.preprocessing import outliers_preprocess\n",
    "from functions.training_pipeline import training_pipeline\n",
    "from functions.models import xgboost_model, catboost_model, lgbm_model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rawdata = 'data/raw_data/'\n",
    "path_models = 'models/proprietary_data/'\n",
    "path_Benchmark = 'Benchmark/'\n",
    "path_results = 'results/proprietary_data/'\n",
    "path_plot = path_results +'plot/'\n",
    "path_intermediary = 'data/intermediary_data/proprietary_data/'\n",
    "path_plot = 'results/proprietary_data/plot/'\n",
    "# ,\"CF3_log\", \"CF123_log\"\n",
    "targets = [\"CF1_log\",\"CF2_log\",\"CF3_log\", \"CF123_log\"]\n",
    "models = {\n",
    "        # \"xgboost\": xgboost_model,\n",
    "        \"catboost\": catboost_model,\n",
    "        \"lgbm\": lgbm_model,\n",
    "}\n",
    "training_parameters = {\n",
    "    \"seed\":0,\n",
    "    \"n_iter\":10,\n",
    "    \"extended_features\": [\n",
    "            \"Revenue_log\",\n",
    "            \"EMP_log\",\n",
    "            \"Asset_log\",\n",
    "            \"NPPE_log\",\n",
    "            \"CapEx_log\",\n",
    "            \"Age\",\n",
    "            \"CapInten\",\n",
    "            \"GMAR\",\n",
    "            \"Leverage\",\n",
    "            \"Price\",\n",
    "            \"FuelIntensity\",\n",
    "            \"FiscalYear\",\n",
    "            \"ENEConsume_log\",\n",
    "            \"ENEProduce_log\",\n",
    "            \"INTAN_log\",\n",
    "            \"AccuDep_log\",\n",
    "            \"COGS_log\",\n",
    "        ],\n",
    "    \"selec_sect\":[\"GICSSubInd\", \"GICSInd\", \"GICSGroup\"],\n",
    "    \"fill_grp\":\"\",\n",
    "    \"old_pipe\":False,  \n",
    "    \"cross_val\": False,\n",
    "}\n",
    "\n",
    "use_weights= None\n",
    "companies=True\n",
    "Summary_Final=[]\n",
    "Summary_Final_train = []\n",
    "ensemble =[]\n",
    "summary_metrics_detailed = pd.DataFrame()\n",
    "estimated_scopes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = load_data(path_rawdata, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset[\"CF1\"] = preprocessed_dataset[\"CF1_merge\"] \n",
    "preprocessed_dataset[\"CF2\"] = preprocessed_dataset[\"CF2_merge\"] \n",
    "preprocessed_dataset[\"CF3\"] = preprocessed_dataset[\"CF3_merge\"] \n",
    "preprocessed_dataset[\"CF123\"] = preprocessed_dataset[\"CF123_merge\"] \n",
    "preprocessed_dataset[\"CDP_CF2\"] = preprocessed_dataset[\"CDP_CF2_location\"]\n",
    "preprocessed_dataset[\"country_sector\"] = preprocessed_dataset[\"CountryHQ\"].astype(str) + \"_\" + preprocessed_dataset[\"GICSSubInd\"].astype(str)\n",
    "# 50 sec\n",
    "threshold_under=1.5\n",
    "threshold_over=2.5\n",
    "for target in [\"CF1_merge\", \"CF2_merge\", \"CF3_merge\", \"CF123_merge\"] : \n",
    "    preprocessed_dataset = outliers_preprocess(preprocessed_dataset, target, threshold_under=threshold_under, threshold_over=threshold_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF1_log\n",
      "Files not found, constructing them\n",
      "preprocessing done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/04 15:14:36 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'catboost' already exists. Creating a new version of this model...\n",
      "2023/08/04 15:14:44 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: catboost, version 67\n",
      "Created version '67' of model 'catboost'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/04 15:14:45 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'lgbm' already exists. Creating a new version of this model...\n",
      "2023/08/04 15:14:48 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: lgbm, version 67\n",
      "Created version '67' of model 'lgbm'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelisation done\n",
      "CF2_log\n",
      "Using pre created preprocessed files\n",
      "preprocessing done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/04 15:14:59 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'catboost' already exists. Creating a new version of this model...\n",
      "2023/08/04 15:15:05 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: catboost, version 68\n",
      "Created version '68' of model 'catboost'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/04 15:15:05 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'lgbm' already exists. Creating a new version of this model...\n",
      "2023/08/04 15:15:09 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: lgbm, version 68\n",
      "Created version '68' of model 'lgbm'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelisation done\n",
      "CF3_log\n",
      "Using pre created preprocessed files\n",
      "preprocessing done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/04 15:15:18 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'catboost' already exists. Creating a new version of this model...\n",
      "2023/08/04 15:15:22 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: catboost, version 69\n",
      "Created version '69' of model 'catboost'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/04 15:15:22 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'lgbm' already exists. Creating a new version of this model...\n",
      "2023/08/04 15:15:26 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: lgbm, version 69\n",
      "Created version '69' of model 'lgbm'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelisation done\n",
      "CF123_log\n",
      "Using pre created preprocessed files\n",
      "preprocessing done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/04 15:15:33 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'catboost' already exists. Creating a new version of this model...\n",
      "2023/08/04 15:15:38 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: catboost, version 70\n",
      "Created version '70' of model 'catboost'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/04 15:15:38 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'lgbm' already exists. Creating a new version of this model...\n",
      "2023/08/04 15:15:43 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: lgbm, version 70\n",
      "Created version '70' of model 'lgbm'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelisation done\n"
     ]
    }
   ],
   "source": [
    "# test de base  \n",
    "targets = [\"CF1_log\",\"CF2_log\",\"CF3_log\", \"CF123_log\"]\n",
    "\n",
    "best_scores, best_stds, summary_global, summary_metrics_detailed = training_pipeline(\n",
    "    name_experiment=\"restriction_CF123_test_base_563232\",\n",
    "    path_Benchmark=path_Benchmark,\n",
    "    path_results=path_results,\n",
    "    path_models=path_models,\n",
    "    path_intermediary=path_intermediary,\n",
    "    path_plot = path_plot,\n",
    "    targets=targets,\n",
    "    models=models,\n",
    "    Summary_Final=Summary_Final,\n",
    "    # Summary_Final_train=Summary_Final_train,\n",
    "    ensemble=ensemble,\n",
    "    summary_metrics_detailed=summary_metrics_detailed,\n",
    "    estimated_scopes = estimated_scopes,\n",
    "    preprocessed_dataset=preprocessed_dataset,\n",
    "    training_parameters=training_parameters,\n",
    "    open_data=False,\n",
    "    save=False,\n",
    "    use_weights=None,\n",
    "    companies=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5233991620098789, 0.40811509811421237, 0.85109105089482, 0.5229286278975259]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores\n",
    "# base \n",
    "# [0.5233991620098789, 0.40811509811421237, 0.85109105089482, 0.5229286278975259]\n",
    "\n",
    "# weights company True\n",
    "# [0.5210419733631437, 0.4159629294599453, 0.8637620192374258, 0.529039907808258]\n",
    "\n",
    "# weights company False\n",
    "# [0.5282566564237224, 0.4276567152521008,  0.8902519108927333, 0.5443683264545418]\n",
    "\n",
    "# weights company False gradient L1\n",
    "\n",
    "# weights company False gradient L2\n",
    "\n",
    "# weights company True gradient L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF1_log\n",
      "Using pre created preprocessed files\n",
      "preprocessing done\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "Invalid weight value type=<class 'numpy.ndarray'>: must be 1 dimensional data with int, float or long types.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# test weights  \u001b[39;00m\n\u001b[0;32m      2\u001b[0m targets \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mCF1_log\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mCF2_log\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mCF3_log\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCF123_log\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m best_scores, best_stds, summary_global, summary_metrics_detailed \u001b[39m=\u001b[39m training_pipeline(\n\u001b[0;32m      5\u001b[0m     name_experiment\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrestriction_CF123_test_gradient_0\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m     path_Benchmark\u001b[39m=\u001b[39;49mpath_Benchmark,\n\u001b[0;32m      7\u001b[0m     path_results\u001b[39m=\u001b[39;49mpath_results,\n\u001b[0;32m      8\u001b[0m     path_models\u001b[39m=\u001b[39;49mpath_models,\n\u001b[0;32m      9\u001b[0m     path_intermediary\u001b[39m=\u001b[39;49mpath_intermediary,\n\u001b[0;32m     10\u001b[0m     path_plot \u001b[39m=\u001b[39;49m path_plot,\n\u001b[0;32m     11\u001b[0m     targets\u001b[39m=\u001b[39;49mtargets,\n\u001b[0;32m     12\u001b[0m     models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m     13\u001b[0m     Summary_Final\u001b[39m=\u001b[39;49mSummary_Final,\n\u001b[0;32m     14\u001b[0m     \u001b[39m# Summary_Final_train=Summary_Final_train,\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m     ensemble\u001b[39m=\u001b[39;49mensemble,\n\u001b[0;32m     16\u001b[0m     summary_metrics_detailed\u001b[39m=\u001b[39;49msummary_metrics_detailed,\n\u001b[0;32m     17\u001b[0m     estimated_scopes \u001b[39m=\u001b[39;49m estimated_scopes,\n\u001b[0;32m     18\u001b[0m     preprocessed_dataset\u001b[39m=\u001b[39;49mpreprocessed_dataset,\n\u001b[0;32m     19\u001b[0m     training_parameters\u001b[39m=\u001b[39;49mtraining_parameters,\n\u001b[0;32m     20\u001b[0m     open_data\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     21\u001b[0m     save\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     22\u001b[0m     use_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     23\u001b[0m     companies\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     24\u001b[0m     custom_gradient\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL1\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# False, \"L1\", \"L2\"\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m )\n",
      "Cell \u001b[1;32mIn[5], line 103\u001b[0m, in \u001b[0;36mtraining_pipeline\u001b[1;34m(name_experiment, path_Benchmark, path_results, path_models, path_intermediary, path_plot, targets, models, Summary_Final, ensemble, summary_metrics_detailed, estimated_scopes, preprocessed_dataset, training_parameters, open_data, save, use_weights, companies, custom_gradient)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mfor\u001b[39;00m i, (model_name, model) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(models\u001b[39m.\u001b[39mitems()):\n\u001b[0;32m    102\u001b[0m     \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run() \u001b[39mas\u001b[39;00m _:\n\u001b[1;32m--> 103\u001b[0m         model_i \u001b[39m=\u001b[39m model(\n\u001b[0;32m    104\u001b[0m             X_train,\n\u001b[0;32m    105\u001b[0m             y_train,\n\u001b[0;32m    106\u001b[0m             cross_val\u001b[39m=\u001b[39;49mtraining_parameters[\u001b[39m\"\u001b[39;49m\u001b[39mcross_val\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    107\u001b[0m             n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    108\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m    109\u001b[0m             n_iter\u001b[39m=\u001b[39;49mn_iter,\n\u001b[0;32m    110\u001b[0m             seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    111\u001b[0m             weights\u001b[39m=\u001b[39;49mweights,\n\u001b[0;32m    112\u001b[0m             custom_gradient\u001b[39m=\u001b[39;49mcustom_gradient,\n\u001b[0;32m    113\u001b[0m         )\n\u001b[0;32m    114\u001b[0m         y_pred \u001b[39m=\u001b[39m model_i\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m    116\u001b[0m         summary_global, rmse, std \u001b[39m=\u001b[39m metrics(y_test, y_pred, Summary_Final, target, model_name)\n",
      "File \u001b[1;32mc:\\Users\\thibaud.barreau\\Desktop\\Projets\\corporate-ghg-emissions-estimations\\functions\\models.py:290\u001b[0m, in \u001b[0;36mcatboost_model\u001b[1;34m(X_train, y_train, cross_val, n_jobs, verbose, n_iter, seed, weights, custom_gradient)\u001b[0m\n\u001b[0;32m    279\u001b[0m     model \u001b[39m=\u001b[39m CatBoostRegressor(\n\u001b[0;32m    280\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    281\u001b[0m         random_state\u001b[39m=\u001b[39mseed,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[39m# iterations=catboost_bo.max[\"params\"][\"iterations\"]\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     )\n\u001b[0;32m    289\u001b[0m \u001b[39mif\u001b[39;00m custom_gradient:\n\u001b[1;32m--> 290\u001b[0m     cat_train_data \u001b[39m=\u001b[39m Pool(data\u001b[39m=\u001b[39;49mX_train, label\u001b[39m=\u001b[39;49my_train, weight\u001b[39m=\u001b[39;49mweights)\n\u001b[0;32m    291\u001b[0m     model\u001b[39m.\u001b[39mfit(cat_train_data)\n\u001b[0;32m    292\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\thibaud.barreau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\catboost\\core.py:792\u001b[0m, in \u001b[0;36mPool.__init__\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature_names, PATH_TYPES):\n\u001b[0;32m    787\u001b[0m             \u001b[39mraise\u001b[39;00m CatBoostError(\n\u001b[0;32m    788\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfeature_names must be None or have non-string type when the pool is created from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    789\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpython objects.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    790\u001b[0m             )\n\u001b[1;32m--> 792\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n\u001b[0;32m    793\u001b[0m                    group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n\u001b[0;32m    794\u001b[0m \u001b[39msuper\u001b[39m(Pool, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\thibaud.barreau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\catboost\\core.py:1391\u001b[0m, in \u001b[0;36mPool._init\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[0;32m   1389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_weight_type(weight)\n\u001b[0;32m   1390\u001b[0m     weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_if_pandas_to_numpy(weight)\n\u001b[1;32m-> 1391\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_weight_shape(weight, samples_count)\n\u001b[0;32m   1392\u001b[0m \u001b[39mif\u001b[39;00m group_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1393\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_group_id_type(group_id)\n",
      "File \u001b[1;32mc:\\Users\\thibaud.barreau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\catboost\\core.py:945\u001b[0m, in \u001b[0;36mPool._check_weight_shape\u001b[1;34m(self, weight, samples_count)\u001b[0m\n\u001b[0;32m    943\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39mLength of weight=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and length of data=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m are different.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(weight), samples_count))\n\u001b[0;32m    944\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(weight[\u001b[39m0\u001b[39m], (INTEGER_TYPES, FLOAT_TYPES)):\n\u001b[1;32m--> 945\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39mInvalid weight value type=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: must be 1 dimensional data with int, float or long types.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(weight[\u001b[39m0\u001b[39m])))\n",
      "\u001b[1;31mCatBoostError\u001b[0m: Invalid weight value type=<class 'numpy.ndarray'>: must be 1 dimensional data with int, float or long types."
     ]
    }
   ],
   "source": [
    "# test weights  \n",
    "targets = [\"CF1_log\",\"CF2_log\",\"CF3_log\", \"CF123_log\"]\n",
    "\n",
    "best_scores, best_stds, summary_global, summary_metrics_detailed = training_pipeline(\n",
    "    name_experiment=\"restriction_CF123_test_gradient_0\",\n",
    "    path_Benchmark=path_Benchmark,\n",
    "    path_results=path_results,\n",
    "    path_models=path_models,\n",
    "    path_intermediary=path_intermediary,\n",
    "    path_plot = path_plot,\n",
    "    targets=targets,\n",
    "    models=models,\n",
    "    Summary_Final=Summary_Final,\n",
    "    # Summary_Final_train=Summary_Final_train,\n",
    "    ensemble=ensemble,\n",
    "    summary_metrics_detailed=summary_metrics_detailed,\n",
    "    estimated_scopes = estimated_scopes,\n",
    "    preprocessed_dataset=preprocessed_dataset,\n",
    "    training_parameters=training_parameters,\n",
    "    open_data=False,\n",
    "    save=False,\n",
    "    use_weights=True,\n",
    "    companies=False,\n",
    "    custom_gradient=\"L1\", # False, \"L1\", \"L2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5210419733631437, 0.4159629294599453, 0.8637620192374258, 0.529039907808258]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target=targets[0]\n",
    "# scope=target[:-4]\n",
    "# a = preprocessed_dataset[[\"FinalEikonID\",\"CDP_\"+scope, scope, \"country_sector\"]]\n",
    "# weights_creation(a, scope, companies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "from functions.preprocessing import custom_train_split\n",
    "from functions.results import best_model_analysis, metrics, results\n",
    "\n",
    "\n",
    "def weights_creation(df, scope, path_intermediary, companies=True):\n",
    "    df[\"weight_reliability\"] = np.ones(len(df))\n",
    "    CDP_indexes = df[df[scope] == df[\"CDP_\" + scope]].index\n",
    "    df.loc[CDP_indexes, \"weight_reliability\"] = [2 for i in range(len(CDP_indexes))]\n",
    "\n",
    "    nb_occurences = df[\"country_sector\"].value_counts()\n",
    "    df[\"weight_country_sector\"] = df.apply(lambda row: 1 / nb_occurences[row[\"country_sector\"]], axis=1)\n",
    "\n",
    "    if companies:\n",
    "        nb_occurences = df.FinalEikonID.value_counts()\n",
    "        df[\"weight_companies\"] = df.apply(lambda row: 1 / nb_occurences[row[\"FinalEikonID\"]], axis=1)\n",
    "\n",
    "        df[\"weight_final\"] = df[\"weight_reliability\"] * df[\"weight_companies\"] * df[\"weight_country_sector\"]\n",
    "    else:\n",
    "        df[\"weight_final\"] = df[\"weight_reliability\"] * df[\"weight_country_sector\"]\n",
    "\n",
    "    return df[\"weight_final\"].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def training_pipeline(\n",
    "    name_experiment,\n",
    "    path_Benchmark,\n",
    "    path_results,\n",
    "    path_models,\n",
    "    path_intermediary,\n",
    "    path_plot,\n",
    "    targets,\n",
    "    models,\n",
    "    Summary_Final,\n",
    "    ensemble,\n",
    "    summary_metrics_detailed,\n",
    "    estimated_scopes,\n",
    "    preprocessed_dataset,\n",
    "    training_parameters,\n",
    "    open_data=False,\n",
    "    save=False,\n",
    "    use_weights=None,\n",
    "    companies=True,\n",
    "    custom_gradient=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply a training pipeline for the imputes targets, models and parameters.\n",
    "    \"\"\"\n",
    "    best_scores = []\n",
    "    best_stds = []\n",
    "    mlflow.create_experiment(\"\" f\"Models_{name_experiment}\")\n",
    "    mlflow.set_experiment(\"\" f\"Models_{name_experiment}\")\n",
    "\n",
    "    for target in targets:\n",
    "        print(target)\n",
    "        test_scores = []\n",
    "        test_stds = []\n",
    "        (\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            df_test,\n",
    "        ) = custom_train_split(\n",
    "            preprocessed_dataset,\n",
    "            path_Benchmark,\n",
    "            path_intermediary,\n",
    "            target,\n",
    "            # threshold_under=training_parameters[\"threshold_under\"],\n",
    "            # threshold_over=training_parameters[\"threshold_over\"],\n",
    "            extended_features=training_parameters[\"extended_features\"],\n",
    "            selec_sect=training_parameters[\"selec_sect\"],\n",
    "            fill_grp=training_parameters[\"fill_grp\"],\n",
    "            old_pipe=training_parameters[\"old_pipe\"],\n",
    "            open_data=open_data,\n",
    "        )\n",
    "        print(\"preprocessing done\")\n",
    "        if use_weights:\n",
    "            scope = target[:-4]\n",
    "            df_train_merged = X_train.join(preprocessed_dataset[[\"FinalEikonID\",\"CDP_\"+scope, scope, \"country_sector\"]])\n",
    "            weights = weights_creation(df_train_merged, scope, companies)\n",
    "            df_train_merged.FinalEikonID.to_csv(\"data/intermediary_data/companies_ids.csv\",index=False)\n",
    "        else:\n",
    "            weights=None\n",
    "\n",
    "        seed = training_parameters[\"seed\"]\n",
    "        n_iter = training_parameters[\"n_iter\"]\n",
    "        for i, (model_name, model) in enumerate(models.items()):\n",
    "            with mlflow.start_run() as _:\n",
    "                model_i = model(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    cross_val=training_parameters[\"cross_val\"],\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0,\n",
    "                    n_iter=n_iter,\n",
    "                    seed=seed,\n",
    "                    weights=weights,\n",
    "                    custom_gradient=custom_gradient,\n",
    "                )\n",
    "                y_pred = model_i.predict(X_test)\n",
    "\n",
    "                summary_global, rmse, std = metrics(y_test, y_pred, Summary_Final, target, model_name)\n",
    "                mlflow.log_metric(\"mae\", mean_absolute_error(y_test, y_pred))\n",
    "                mlflow.log_metric(\"rmse\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "                mlflow.log_metric(\"mse\", mean_squared_error(y_test, y_pred))\n",
    "                mlflow.log_metric(\"r2\", r2_score(y_test, y_pred))\n",
    "                mlflow.log_metric(\"mape\", mean_absolute_percentage_error(y_test, y_pred))\n",
    "                mlflow.log_param(\"target\", target)\n",
    "                mlflow.log_param(\"model\", model_name)\n",
    "                mlflow.sklearn.log_model(model, \"models\", registered_model_name=model_name)\n",
    "                ensemble.append(model_i)\n",
    "                # model_name_lst.append(model_name)\n",
    "                test_scores.append(rmse)\n",
    "                test_stds.append(std)\n",
    "\n",
    "        best_scores.append(test_scores[test_scores.index(min(test_scores))])\n",
    "        best_stds.append(test_stds[test_scores.index(min(test_scores))])\n",
    "        print(\"modelisation done\")\n",
    "\n",
    "        if save:\n",
    "            best_model_index = test_scores.index(min(test_scores))\n",
    "            best_model = ensemble[best_model_index]\n",
    "            summary_metrics_detailed, estimated_scopes, lst = best_model_analysis(\n",
    "                best_model,\n",
    "                X_test,\n",
    "                X_train,\n",
    "                y_test,\n",
    "                df_test,\n",
    "                target,\n",
    "                path_plot,\n",
    "                preprocessed_dataset,\n",
    "                path_intermediary,\n",
    "                summary_metrics_detailed,\n",
    "                estimated_scopes,\n",
    "                training_parameters,\n",
    "                open_data,\n",
    "                path_models,\n",
    "            )\n",
    "\n",
    "            results(estimated_scopes, path_results, summary_metrics_detailed, Summary_Final, lst)\n",
    "\n",
    "    return best_scores, best_stds, summary_global, summary_metrics_detailed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_gradient=\"L1\"\n",
    "use_weights=True\n",
    "best_scores = []\n",
    "best_stds = []\n",
    "models = {\n",
    "        # \"xgboost\": xgboost_model,\n",
    "        \"catboost\": catboost_model,\n",
    "        \"lgbm\": lgbm_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF1_log\n",
      "Using pre created preprocessed files\n",
      "preprocessing done\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m i, (model_name, model) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(models\u001b[39m.\u001b[39mitems()):\n\u001b[0;32m     32\u001b[0m     \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run() \u001b[39mas\u001b[39;00m _:\n\u001b[1;32m---> 33\u001b[0m         model_i \u001b[39m=\u001b[39m model(\n\u001b[0;32m     34\u001b[0m             X_train,\n\u001b[0;32m     35\u001b[0m             y_train,\n\u001b[0;32m     36\u001b[0m             cross_val\u001b[39m=\u001b[39;49mtraining_parameters[\u001b[39m\"\u001b[39;49m\u001b[39mcross_val\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     37\u001b[0m             n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     38\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     39\u001b[0m             n_iter\u001b[39m=\u001b[39;49mn_iter,\n\u001b[0;32m     40\u001b[0m             seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m     41\u001b[0m             weights\u001b[39m=\u001b[39;49mweights,\n\u001b[0;32m     42\u001b[0m             custom_gradient\u001b[39m=\u001b[39;49mcustom_gradient,\n\u001b[0;32m     43\u001b[0m         )\n\u001b[0;32m     44\u001b[0m         y_pred \u001b[39m=\u001b[39m model_i\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     46\u001b[0m         summary_global, rmse, std \u001b[39m=\u001b[39m metrics(y_test, y_pred, Summary_Final, target, model_name)\n",
      "File \u001b[1;32mc:\\Users\\thibaud.barreau\\Desktop\\Projets\\corporate-ghg-emissions-estimations\\functions\\models.py:190\u001b[0m, in \u001b[0;36mlgbm_model\u001b[1;34m(X_train, y_train, cross_val, n_jobs, verbose, n_iter, seed, weights, custom_gradient)\u001b[0m\n\u001b[0;32m    178\u001b[0m     model \u001b[39m=\u001b[39m LGBMRegressor(\n\u001b[0;32m    179\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    180\u001b[0m         random_state\u001b[39m=\u001b[39mseed,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m         max_depth\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(lgbm_bo\u001b[39m.\u001b[39mmax[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m custom_gradient:\n\u001b[1;32m--> 190\u001b[0m     model\u001b[39m.\u001b[39mfit(\n\u001b[0;32m    191\u001b[0m         X_train,\n\u001b[0;32m    192\u001b[0m         y_train,\n\u001b[0;32m    193\u001b[0m         eval_metric\u001b[39m=\u001b[39mrmse_metric_lgbm,\n\u001b[0;32m    194\u001b[0m         sample_weight\u001b[39m=\u001b[39mweights,\n\u001b[0;32m    195\u001b[0m         eval_sample_weight\u001b[39m=\u001b[39mweights,\n\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\thibaud.barreau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:1764\u001b[0m, in \u001b[0;36mDataFrame.from_dict\u001b[1;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[0;32m   1758\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1759\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected \u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtight\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for orient parameter. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1760\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00morient\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1761\u001b[0m     )\n\u001b[0;32m   1763\u001b[0m \u001b[39mif\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtight\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 1764\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(data, index\u001b[39m=\u001b[39;49mindex, columns\u001b[39m=\u001b[39;49mcolumns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m   1765\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1766\u001b[0m     realdata \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\thibaud.barreau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thibaud.barreau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\thibaud.barreau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\thibaud.barreau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:656\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    655\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indexes \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 656\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf using all scalar values, you must pass an index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    658\u001b[0m \u001b[39melif\u001b[39;00m have_series:\n\u001b[0;32m    659\u001b[0m     index \u001b[39m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "for target in targets:\n",
    "    print(target)\n",
    "    test_scores = []\n",
    "    test_stds = []\n",
    "    (\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        df_test,\n",
    "    ) = custom_train_split(\n",
    "        preprocessed_dataset,\n",
    "        path_Benchmark,\n",
    "        path_intermediary,\n",
    "        target,\n",
    "        extended_features=training_parameters[\"extended_features\"],\n",
    "        selec_sect=training_parameters[\"selec_sect\"],\n",
    "        fill_grp=training_parameters[\"fill_grp\"],\n",
    "        old_pipe=training_parameters[\"old_pipe\"],\n",
    "        open_data=False,\n",
    "    )\n",
    "    print(\"preprocessing done\")\n",
    "    if use_weights:\n",
    "        scope = target[:-4]\n",
    "        df_train_merged = X_train.join(preprocessed_dataset[[\"FinalEikonID\",\"CDP_\"+scope, scope, \"country_sector\"]]) \n",
    "        weights = weights_creation(df_train_merged, scope, companies)\n",
    "        df_train_merged.FinalEikonID.to_csv(\"data/intermediary_data/companies_ids.csv\",index=False)\n",
    "\n",
    "    seed = training_parameters[\"seed\"]\n",
    "    n_iter = training_parameters[\"n_iter\"]\n",
    "    for i, (model_name, model) in enumerate(models.items()):\n",
    "        with mlflow.start_run() as _:\n",
    "            model_i = model(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                cross_val=training_parameters[\"cross_val\"],\n",
    "                n_jobs=-1,\n",
    "                verbose=0,\n",
    "                n_iter=n_iter,\n",
    "                seed=seed,\n",
    "                weights=weights,\n",
    "                custom_gradient=custom_gradient,\n",
    "            )\n",
    "            y_pred = model_i.predict(X_test)\n",
    "\n",
    "            summary_global, rmse, std = metrics(y_test, y_pred, Summary_Final, target, model_name)\n",
    "            mlflow.log_metric(\"mae\", mean_absolute_error(y_test, y_pred))\n",
    "            mlflow.log_metric(\"rmse\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "            mlflow.log_metric(\"mse\", mean_squared_error(y_test, y_pred))\n",
    "            mlflow.log_metric(\"r2\", r2_score(y_test, y_pred))\n",
    "            mlflow.log_metric(\"mape\", mean_absolute_percentage_error(y_test, y_pred))\n",
    "            mlflow.log_param(\"target\", target)\n",
    "            mlflow.log_param(\"model\", model_name)\n",
    "            mlflow.sklearn.log_model(model, \"models\", registered_model_name=model_name)\n",
    "            ensemble.append(model_i)\n",
    "            # model_name_lst.append(model_name)\n",
    "            test_scores.append(rmse)\n",
    "            test_stds.append(std)\n",
    "\n",
    "    best_scores.append(test_scores[test_scores.index(min(test_scores))])\n",
    "    best_stds.append(test_stds[test_scores.index(min(test_scores))])\n",
    "    print(\"modelisation done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FinalEikonID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0812.HK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPO.L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IPO.L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGNC.OQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGNC.OQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22304</th>\n",
       "      <td>FWONA.OQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22305</th>\n",
       "      <td>FWONA.OQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22306</th>\n",
       "      <td>FWONA.OQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22307</th>\n",
       "      <td>MAHAa.ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22308</th>\n",
       "      <td>MAHAa.ST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22309 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FinalEikonID\n",
       "0          0812.HK\n",
       "1            IPO.L\n",
       "2            IPO.L\n",
       "3          AGNC.OQ\n",
       "4          AGNC.OQ\n",
       "...            ...\n",
       "22304     FWONA.OQ\n",
       "22305     FWONA.OQ\n",
       "22306     FWONA.OQ\n",
       "22307     MAHAa.ST\n",
       "22308     MAHAa.ST\n",
       "\n",
       "[22309 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# companies_ids = pd.read_csv(\"data/intermediary_data/companies_ids.csv\")\n",
    "companies_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
